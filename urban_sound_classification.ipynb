{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and its structure\n",
    "\n",
    "1. We can use Urban Sound Classification ( https://urbansounddataset.weebly.com/ ) dataset which is quite popular.\n",
    "2. Whichever dataset you are using, it is important to understand its structure and how to extract required features out of them.\n",
    "3. For UrbanSound8K dataset, it can be downloaded using the following link ( https://goo.gl/8hY5ER  ). It downloads a compressed tar file of size around 6GB.\n",
    "4. On extracting it, it contains two folders named 'audio' and 'metadata'.\n",
    "5. Audio folder contains 10 folders with name fold1, fold2 and so on, each having approximately 800 audio files of 4s each.\n",
    "6. Metadata folder contains a .csv file having various columns such as file_id, label, class_id corresponding to label, salience etc.\n",
    "7. Complete description can be found here https://urbansounddataset.weebly.com/urbansound8k.html\n",
    "\n",
    "## Research Paper and Resources to follow\n",
    "\n",
    "1. https://github.com/meyda/meyda/wiki/audio-features\n",
    "2. https://github.com/tyiannak/pyAudioAnalysis/wiki/3.-Feature-Extraction\n",
    "3. https://medium.com/@ageitgey/machine-learning-is-fun-part-6-how-to-do-speech-recognition-with-deep-learning-28293c162f7a\n",
    "4. https://towardsdatascience.com/urban-sound-classification-part-1-99137c6335f9\n",
    "5. https://www.analyticsvidhya.com/blog/2017/08/audio-voice-processing-deep-learning/\n",
    "\n",
    "## Library To Use\n",
    "\n",
    "We can use librosa library which can be installed using \n",
    "> pip install librosa\n",
    "\n",
    "It uses ffmpeg as backend to convert and read some of the audio files. So to install ffmpeg, you can use \n",
    "> apt-get install ffmpeg\n",
    "\n",
    "Librosa library can read audio files and convert them to there amplitude values for each sample of audio. Let us say there is an audio file of 4s and sampling rate of audio file is 22050 Hz. This means that audio file is made using amplitude samples such that 22050 samples of amplitudes are recorded in each second. Hence a 4s audio file with sampling rate 22050 can be expressed as an array of 4\\*22050=88200 size \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to Load Audio Files and Extract Features\n",
    "\n",
    "Using load method of librosa library, we can read audio files. It takes file path as input and returns an array having amplitude samples along with sampling rate of file.\n",
    "\n",
    "Librosa library has many methods already build to extract features mentioned in resources which then returns another array of features.\n",
    "We can use various combinations of those features. This is something you can play around and try how and which features like mfcc, spectral features, energy etc affect the classification of audio. \n",
    "\n",
    "For eg, in first stage you can extract only mfcc features and then build up a model and check the accuracy. Then try the same with other features. In order to further improve accuracy, you can also try to use more than one type of features and check the results.\n",
    "\n",
    "## Using CNN to classify sound\n",
    "\n",
    "This is a very classical way of sound classification as it is observed that similar type of sounds have similar spectrogram (read resource 3 to understand more about spectrogram). A spectrogram is a visual representation of the spectrum of frequencies of sound or other signal as they vary with time. And thus we can train a CNN network which takes these spectrogram images as input and using it tries to generalize patterns and hence classify them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting librosa\n",
      "  Downloading librosa-0.9.2-py3-none-any.whl (214 kB)\n",
      "Requirement already satisfied: joblib>=0.14 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (1.1.0)\n",
      "Requirement already satisfied: decorator>=4.0.10 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (5.1.0)\n",
      "Collecting soundfile>=0.10.2\n",
      "  Downloading soundfile-0.11.0-py2.py3-none-win_amd64.whl (1.0 MB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (21.0)\n",
      "Collecting resampy>=0.2.2\n",
      "  Downloading resampy-0.4.2-py3-none-any.whl (3.1 MB)\n",
      "Requirement already satisfied: numba>=0.45.1 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (0.54.1)\n",
      "Requirement already satisfied: scipy>=1.2.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (1.7.1)\n",
      "Collecting audioread>=2.1.9\n",
      "  Downloading audioread-3.0.0.tar.gz (377 kB)\n",
      "Collecting pooch>=1.0\n",
      "  Downloading pooch-1.6.0-py3-none-any.whl (56 kB)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (0.24.2)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from librosa) (1.20.3)\n",
      "Requirement already satisfied: setuptools in c:\\users\\drish\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (58.0.4)\n",
      "Requirement already satisfied: llvmlite<0.38,>=0.37.0rc1 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from numba>=0.45.1->librosa) (0.37.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from packaging>=20.0->librosa) (3.0.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (2.26.0)\n",
      "Requirement already satisfied: appdirs>=1.3.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from pooch>=1.0->librosa) (1.4.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (1.26.7)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from requests>=2.19.0->pooch>=1.0->librosa) (3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from scikit-learn>=0.19.1->librosa) (2.2.0)\n",
      "Requirement already satisfied: cffi>=1.0 in c:\\users\\drish\\anaconda3\\lib\\site-packages (from soundfile>=0.10.2->librosa) (1.14.6)\n",
      "Requirement already satisfied: pycparser in c:\\users\\drish\\anaconda3\\lib\\site-packages (from cffi>=1.0->soundfile>=0.10.2->librosa) (2.20)\n",
      "Building wheels for collected packages: audioread\n",
      "  Building wheel for audioread (setup.py): started\n",
      "  Building wheel for audioread (setup.py): finished with status 'done'\n",
      "  Created wheel for audioread: filename=audioread-3.0.0-py3-none-any.whl size=23706 sha256=03eebb651f0a80e5c428dc689314ecdbfe861941ae68932523e4b9bf4f0e8827\n",
      "  Stored in directory: c:\\users\\drish\\appdata\\local\\pip\\cache\\wheels\\e4\\76\\a4\\cfb55573167a1f5bde7d7a348e95e509c64b2c3e8f921932c3\n",
      "Successfully built audioread\n",
      "Installing collected packages: soundfile, resampy, pooch, audioread, librosa\n",
      "Successfully installed audioread-3.0.0 librosa-0.9.2 pooch-1.6.0 resampy-0.4.2 soundfile-0.11.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install librosa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-pythonNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Requirement already satisfied: future in c:\\users\\drish\\anaconda3\\lib\\site-packages (from ffmpeg-python) (0.18.2)\n",
      "Installing collected packages: ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Volume in drive C is Windows\n",
      " Volume Serial Number is 7061-83D3\n",
      "\n",
      " Directory of C:\\Users\\drish\\Downloads\\Coding Ninjas ML\\Urban Sound Classification\n",
      "\n",
      "21-10-2022  19:26    <DIR>          .\n",
      "19-10-2022  15:16    <DIR>          ..\n",
      "19-10-2022  15:19    <DIR>          .ipynb_checkpoints\n",
      "21-10-2022  17:46    <DIR>          UrbanSound8K\n",
      "04-06-2014  03:46     7,097,425,920 UrbanSound8K.tar\n",
      "21-10-2022  17:37     6,023,741,708 UrbanSound8K.tar.gz\n",
      "21-10-2022  19:26            10,843 urban_sound_classification.ipynb\n",
      "               3 File(s) 13,121,178,471 bytes\n",
      "               4 Dir(s)  326,412,599,296 bytes free\n"
     ]
    }
   ],
   "source": [
    "!dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slice_file_name</th>\n",
       "      <th>fsID</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>salience</th>\n",
       "      <th>fold</th>\n",
       "      <th>classID</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100032-3-0-0.wav</td>\n",
       "      <td>100032</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.317551</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100263-2-0-117.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>58.5</td>\n",
       "      <td>62.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100263-2-0-121.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>60.5</td>\n",
       "      <td>64.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100263-2-0-126.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>63.0</td>\n",
       "      <td>67.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100263-2-0-137.wav</td>\n",
       "      <td>100263</td>\n",
       "      <td>68.5</td>\n",
       "      <td>72.500000</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      slice_file_name    fsID  start        end  salience  fold  classID  \\\n",
       "0    100032-3-0-0.wav  100032    0.0   0.317551         1     5        3   \n",
       "1  100263-2-0-117.wav  100263   58.5  62.500000         1     5        2   \n",
       "2  100263-2-0-121.wav  100263   60.5  64.500000         1     5        2   \n",
       "3  100263-2-0-126.wav  100263   63.0  67.000000         1     5        2   \n",
       "4  100263-2-0-137.wav  100263   68.5  72.500000         1     5        2   \n",
       "\n",
       "              class  \n",
       "0          dog_bark  \n",
       "1  children_playing  \n",
       "2  children_playing  \n",
       "3  children_playing  \n",
       "4  children_playing  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "audio_dataset_path=\"UrbanSound8K/UrbanSound8K/audio/\"\n",
    "metadata=pd.read_csv(\"UrbanSound8K/UrbanSound8K/metadata/UrbanSound8K.csv\")\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MFCC\n",
    "It generates features from a given audio file from its time and frequency characteristics\n",
    "We are using librosa which will load all audio files in sampling rate of 22khz approx for simplicity. Librosa wil provide audio data between -1 and +1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def features_extractor(file):\n",
    "    audio,sample_rate=librosa.load(file,res_type=\"kaiser_fast\")\n",
    "    mfcc_features=librosa.feature.mfcc(y=audio,sr=sample_rate,n_mfcc=40)\n",
    "    mfcc_scaled_features=np.mean(mfcc_features.T,axis=0)\n",
    "    return mfcc_scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3555it [03:10, 18.49it/s]C:\\Users\\drish\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1323\n",
      "  return f(*args, **kwargs)\n",
      "8324it [07:11, 26.14it/s]C:\\Users\\drish\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1103\n",
      "  return f(*args, **kwargs)\n",
      "C:\\Users\\drish\\anaconda3\\lib\\site-packages\\librosa\\util\\decorators.py:88: UserWarning: n_fft=2048 is too small for input signal of length=1523\n",
      "  return f(*args, **kwargs)\n",
      "8732it [07:31, 19.34it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "##Iterate through all files and extract features using above function\n",
    "extracted_features=[]\n",
    "for index_num,row in tqdm(metadata.iterrows()):\n",
    "    file_name=os.path.join(os.path.abspath(audio_dataset_path),\"fold\"+str(row[\"fold\"])+\"/\"+str(row[\"slice_file_name\"]))\n",
    "    final_class_labels=row[\"class\"]\n",
    "    data=features_extractor(file_name)\n",
    "    extracted_features.append([data,final_class_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[-217.35526, 70.22339, -130.38527, -53.282898,...</td>\n",
       "      <td>dog_bark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[-424.09818, 109.34077, -52.919525, 60.86475, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[-458.79114, 121.38419, -46.520657, 52.00812, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[-413.89984, 101.66373, -35.42945, 53.036358, ...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[-446.60352, 113.68541, -52.402206, 60.302044,...</td>\n",
       "      <td>children_playing</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             feature             class\n",
       "0  [-217.35526, 70.22339, -130.38527, -53.282898,...          dog_bark\n",
       "1  [-424.09818, 109.34077, -52.919525, 60.86475, ...  children_playing\n",
       "2  [-458.79114, 121.38419, -46.520657, 52.00812, ...  children_playing\n",
       "3  [-413.89984, 101.66373, -35.42945, 53.036358, ...  children_playing\n",
       "4  [-446.60352, 113.68541, -52.402206, 60.302044,...  children_playing"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_features_df=pd.DataFrame(extracted_features,columns=[\"feature\",\"class\"])\n",
    "extracted_features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=np.array(extracted_features_df[\"feature\"].to_list())\n",
    "y=np.array(extracted_features_df[\"class\"].to_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 40)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Label Encoding for CNN\n",
    "y=np.array(pd.get_dummies(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8732, 10)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((6985, 40), (6985, 10))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape,Y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1747, 40), (1747, 10))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape,Y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense,Dropout,Activation,Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_labels=Y_test.shape[1]\n",
    "num_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "#first layer\n",
    "model.add(Dense(256,input_shape=(40,))) #because 40 features\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#second layer\n",
    "model.add(Dense(512)) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#third layer\n",
    "model.add(Dense(1024)) \n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#final layer\n",
    "model.add(Dense(num_labels))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_7 (Dense)             (None, 256)               10496     \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 256)               0         \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 512)               131584    \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 512)               0         \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 512)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 1024)              525312    \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 1024)              0         \n",
      "                                                                 \n",
      " dropout_8 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 10)                10250     \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 10)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 677,642\n",
      "Trainable params: 677,642\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\",metrics=[\"accuracy\"],optimizer=\"Adam\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "219/219 [==============================] - 4s 13ms/step - loss: 6.7213 - accuracy: 0.1943 - val_loss: 2.0262 - val_accuracy: 0.2490\n",
      "Epoch 2/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 2.1705 - accuracy: 0.2800 - val_loss: 1.8565 - val_accuracy: 0.4110\n",
      "Epoch 3/100\n",
      "219/219 [==============================] - 2s 11ms/step - loss: 1.8671 - accuracy: 0.3535 - val_loss: 1.7357 - val_accuracy: 0.4327\n",
      "Epoch 4/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.7254 - accuracy: 0.4011 - val_loss: 1.5264 - val_accuracy: 0.5266\n",
      "Epoch 5/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.5622 - accuracy: 0.4567 - val_loss: 1.3950 - val_accuracy: 0.5541\n",
      "Epoch 6/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.4730 - accuracy: 0.4929 - val_loss: 1.3099 - val_accuracy: 0.5844\n",
      "Epoch 7/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.3869 - accuracy: 0.5184 - val_loss: 1.1911 - val_accuracy: 0.6365\n",
      "Epoch 8/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 1.3078 - accuracy: 0.5593 - val_loss: 1.1286 - val_accuracy: 0.6308\n",
      "Epoch 9/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 1.2437 - accuracy: 0.5837 - val_loss: 1.0318 - val_accuracy: 0.6651\n",
      "Epoch 10/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.2017 - accuracy: 0.5936 - val_loss: 1.0025 - val_accuracy: 0.6726\n",
      "Epoch 11/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.1674 - accuracy: 0.5984 - val_loss: 0.9744 - val_accuracy: 0.7035\n",
      "Epoch 12/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 1.1304 - accuracy: 0.6189 - val_loss: 0.9634 - val_accuracy: 0.6955\n",
      "Epoch 13/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 1.1015 - accuracy: 0.6218 - val_loss: 0.9144 - val_accuracy: 0.7149\n",
      "Epoch 14/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.0513 - accuracy: 0.6492 - val_loss: 0.8453 - val_accuracy: 0.7430\n",
      "Epoch 15/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 1.0380 - accuracy: 0.6451 - val_loss: 0.8396 - val_accuracy: 0.7407\n",
      "Epoch 16/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.9768 - accuracy: 0.6754 - val_loss: 0.8089 - val_accuracy: 0.7590\n",
      "Epoch 17/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.9597 - accuracy: 0.6789 - val_loss: 0.7838 - val_accuracy: 0.7476\n",
      "Epoch 18/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.9567 - accuracy: 0.6746 - val_loss: 0.7604 - val_accuracy: 0.7739\n",
      "Epoch 19/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.9356 - accuracy: 0.6876 - val_loss: 0.7516 - val_accuracy: 0.7785\n",
      "Epoch 20/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.9264 - accuracy: 0.6928 - val_loss: 0.7542 - val_accuracy: 0.7607\n",
      "Epoch 21/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.9000 - accuracy: 0.6942 - val_loss: 0.7117 - val_accuracy: 0.7859\n",
      "Epoch 22/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8510 - accuracy: 0.7095 - val_loss: 0.7107 - val_accuracy: 0.7848\n",
      "Epoch 23/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.8793 - accuracy: 0.7048 - val_loss: 0.7074 - val_accuracy: 0.7796\n",
      "Epoch 24/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.8860 - accuracy: 0.7061 - val_loss: 0.7426 - val_accuracy: 0.7802\n",
      "Epoch 25/100\n",
      "219/219 [==============================] - 2s 10ms/step - loss: 0.8592 - accuracy: 0.7181 - val_loss: 0.6739 - val_accuracy: 0.7974\n",
      "Epoch 26/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8501 - accuracy: 0.7177 - val_loss: 0.6753 - val_accuracy: 0.7974\n",
      "Epoch 27/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8121 - accuracy: 0.7317 - val_loss: 0.6355 - val_accuracy: 0.8060\n",
      "Epoch 28/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8175 - accuracy: 0.7309 - val_loss: 0.6016 - val_accuracy: 0.8180\n",
      "Epoch 29/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8274 - accuracy: 0.7281 - val_loss: 0.6562 - val_accuracy: 0.8128\n",
      "Epoch 30/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8198 - accuracy: 0.7283 - val_loss: 0.6348 - val_accuracy: 0.8145\n",
      "Epoch 31/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8027 - accuracy: 0.7350 - val_loss: 0.6312 - val_accuracy: 0.8025\n",
      "Epoch 32/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.8139 - accuracy: 0.7228 - val_loss: 0.6189 - val_accuracy: 0.8111\n",
      "Epoch 33/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7803 - accuracy: 0.7390 - val_loss: 0.6030 - val_accuracy: 0.8197\n",
      "Epoch 34/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7651 - accuracy: 0.7426 - val_loss: 0.6094 - val_accuracy: 0.8168\n",
      "Epoch 35/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7885 - accuracy: 0.7414 - val_loss: 0.5930 - val_accuracy: 0.8243\n",
      "Epoch 36/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7951 - accuracy: 0.7366 - val_loss: 0.6165 - val_accuracy: 0.8060\n",
      "Epoch 37/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7720 - accuracy: 0.7530 - val_loss: 0.6156 - val_accuracy: 0.8157\n",
      "Epoch 38/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7437 - accuracy: 0.7528 - val_loss: 0.6049 - val_accuracy: 0.8220\n",
      "Epoch 39/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7470 - accuracy: 0.7562 - val_loss: 0.5893 - val_accuracy: 0.8180\n",
      "Epoch 40/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7412 - accuracy: 0.7598 - val_loss: 0.5925 - val_accuracy: 0.8203\n",
      "Epoch 41/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7522 - accuracy: 0.7536 - val_loss: 0.5843 - val_accuracy: 0.8231\n",
      "Epoch 42/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7519 - accuracy: 0.7596 - val_loss: 0.5953 - val_accuracy: 0.8208\n",
      "Epoch 43/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7574 - accuracy: 0.7545 - val_loss: 0.5731 - val_accuracy: 0.8340\n",
      "Epoch 44/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7552 - accuracy: 0.7470 - val_loss: 0.5779 - val_accuracy: 0.8311\n",
      "Epoch 45/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7108 - accuracy: 0.7636 - val_loss: 0.5735 - val_accuracy: 0.8197\n",
      "Epoch 46/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7536 - accuracy: 0.7559 - val_loss: 0.5722 - val_accuracy: 0.8288\n",
      "Epoch 47/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7354 - accuracy: 0.7639 - val_loss: 0.5797 - val_accuracy: 0.8266\n",
      "Epoch 48/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7084 - accuracy: 0.7691 - val_loss: 0.5397 - val_accuracy: 0.8363\n",
      "Epoch 49/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7256 - accuracy: 0.7621 - val_loss: 0.5833 - val_accuracy: 0.8197\n",
      "Epoch 50/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7114 - accuracy: 0.7674 - val_loss: 0.5534 - val_accuracy: 0.8317\n",
      "Epoch 51/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7353 - accuracy: 0.7611 - val_loss: 0.5595 - val_accuracy: 0.8311\n",
      "Epoch 52/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7284 - accuracy: 0.7669 - val_loss: 0.5791 - val_accuracy: 0.8300\n",
      "Epoch 53/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7340 - accuracy: 0.7571 - val_loss: 0.5765 - val_accuracy: 0.8214\n",
      "Epoch 54/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7160 - accuracy: 0.7674 - val_loss: 0.5710 - val_accuracy: 0.8334\n",
      "Epoch 55/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7148 - accuracy: 0.7689 - val_loss: 0.5489 - val_accuracy: 0.8266\n",
      "Epoch 56/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7105 - accuracy: 0.7686 - val_loss: 0.5609 - val_accuracy: 0.8357\n",
      "Epoch 57/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7001 - accuracy: 0.7696 - val_loss: 0.5653 - val_accuracy: 0.8266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 58/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7192 - accuracy: 0.7678 - val_loss: 0.5902 - val_accuracy: 0.8254\n",
      "Epoch 59/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7121 - accuracy: 0.7725 - val_loss: 0.5491 - val_accuracy: 0.8414\n",
      "Epoch 60/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6992 - accuracy: 0.7764 - val_loss: 0.5330 - val_accuracy: 0.8483\n",
      "Epoch 61/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6996 - accuracy: 0.7775 - val_loss: 0.5461 - val_accuracy: 0.8369\n",
      "Epoch 62/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6758 - accuracy: 0.7804 - val_loss: 0.5373 - val_accuracy: 0.8432\n",
      "Epoch 63/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7092 - accuracy: 0.7696 - val_loss: 0.5854 - val_accuracy: 0.8311\n",
      "Epoch 64/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6700 - accuracy: 0.7841 - val_loss: 0.5383 - val_accuracy: 0.8386\n",
      "Epoch 65/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7109 - accuracy: 0.7734 - val_loss: 0.5288 - val_accuracy: 0.8575\n",
      "Epoch 66/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7031 - accuracy: 0.7741 - val_loss: 0.5424 - val_accuracy: 0.8403\n",
      "Epoch 67/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6890 - accuracy: 0.7775 - val_loss: 0.5203 - val_accuracy: 0.8454\n",
      "Epoch 68/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6986 - accuracy: 0.7704 - val_loss: 0.5514 - val_accuracy: 0.8283\n",
      "Epoch 69/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7085 - accuracy: 0.7792 - val_loss: 0.5512 - val_accuracy: 0.8311\n",
      "Epoch 70/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.7069 - accuracy: 0.7770 - val_loss: 0.5444 - val_accuracy: 0.8323\n",
      "Epoch 71/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6938 - accuracy: 0.7817 - val_loss: 0.5733 - val_accuracy: 0.8300\n",
      "Epoch 72/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6701 - accuracy: 0.7907 - val_loss: 0.5377 - val_accuracy: 0.8351\n",
      "Epoch 73/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6496 - accuracy: 0.7937 - val_loss: 0.5279 - val_accuracy: 0.8374\n",
      "Epoch 74/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6569 - accuracy: 0.7885 - val_loss: 0.5484 - val_accuracy: 0.8443\n",
      "Epoch 75/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6802 - accuracy: 0.7844 - val_loss: 0.5407 - val_accuracy: 0.8517\n",
      "Epoch 76/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6889 - accuracy: 0.7858 - val_loss: 0.5216 - val_accuracy: 0.8512\n",
      "Epoch 77/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6762 - accuracy: 0.7845 - val_loss: 0.5409 - val_accuracy: 0.8351\n",
      "Epoch 78/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6552 - accuracy: 0.7878 - val_loss: 0.5429 - val_accuracy: 0.8409\n",
      "Epoch 79/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6758 - accuracy: 0.7873 - val_loss: 0.5228 - val_accuracy: 0.8523\n",
      "Epoch 80/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6726 - accuracy: 0.7825 - val_loss: 0.5337 - val_accuracy: 0.8351\n",
      "Epoch 81/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6504 - accuracy: 0.7918 - val_loss: 0.5377 - val_accuracy: 0.8334\n",
      "Epoch 82/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6510 - accuracy: 0.7937 - val_loss: 0.5399 - val_accuracy: 0.8426\n",
      "Epoch 83/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6644 - accuracy: 0.7895 - val_loss: 0.5328 - val_accuracy: 0.8306\n",
      "Epoch 84/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6522 - accuracy: 0.7887 - val_loss: 0.5533 - val_accuracy: 0.8380\n",
      "Epoch 85/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6712 - accuracy: 0.7865 - val_loss: 0.5382 - val_accuracy: 0.8443\n",
      "Epoch 86/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6603 - accuracy: 0.7941 - val_loss: 0.5499 - val_accuracy: 0.8426\n",
      "Epoch 87/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6815 - accuracy: 0.7885 - val_loss: 0.5451 - val_accuracy: 0.8535\n",
      "Epoch 88/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6602 - accuracy: 0.7928 - val_loss: 0.5563 - val_accuracy: 0.8414\n",
      "Epoch 89/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6481 - accuracy: 0.7946 - val_loss: 0.5238 - val_accuracy: 0.8369\n",
      "Epoch 90/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6625 - accuracy: 0.7884 - val_loss: 0.5321 - val_accuracy: 0.8403\n",
      "Epoch 91/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6475 - accuracy: 0.7936 - val_loss: 0.4863 - val_accuracy: 0.8569\n",
      "Epoch 92/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6582 - accuracy: 0.7947 - val_loss: 0.5159 - val_accuracy: 0.8563\n",
      "Epoch 93/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6460 - accuracy: 0.7923 - val_loss: 0.5135 - val_accuracy: 0.8558\n",
      "Epoch 94/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6681 - accuracy: 0.7934 - val_loss: 0.5063 - val_accuracy: 0.8638\n",
      "Epoch 95/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6442 - accuracy: 0.7941 - val_loss: 0.4950 - val_accuracy: 0.8558\n",
      "Epoch 96/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6576 - accuracy: 0.7997 - val_loss: 0.5328 - val_accuracy: 0.8432\n",
      "Epoch 97/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6396 - accuracy: 0.7940 - val_loss: 0.5075 - val_accuracy: 0.8563\n",
      "Epoch 98/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6828 - accuracy: 0.7900 - val_loss: 0.5517 - val_accuracy: 0.8363\n",
      "Epoch 99/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6378 - accuracy: 0.7976 - val_loss: 0.5070 - val_accuracy: 0.8529\n",
      "Epoch 100/100\n",
      "219/219 [==============================] - 2s 9ms/step - loss: 0.6546 - accuracy: 0.7943 - val_loss: 0.5222 - val_accuracy: 0.8495\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x190ffa3d430>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train,Y_train,batch_size=32,epochs=100,validation_data=(X_test,Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8494561910629272\n"
     ]
    }
   ],
   "source": [
    "test_accuracy=model.evaluate(X_test,Y_test,verbose=0)\n",
    "print(test_accuracy[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
